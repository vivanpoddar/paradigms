const fs = require('fs');
const path = require('path');

// Test the batch processing endpoint
async function testBatchProcessing() {
    try {
        console.log('ğŸš€ Testing Document AI Batch Processing...');
        
        // Test data - replace with actual values from your Supabase storage
        const testData = {
            fileName: 'test-document.pdf',
            bucketName: 'documents', // Replace with your actual bucket name
            uploadPath: 'path/to/your/test-document.pdf', // Replace with actual path
            userId: 'test-user-id' // Replace with actual user ID
        };

        // Make API call to the nparse endpoint
        const response = await fetch('http://localhost:3000/api/nparse', {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json',
            },
            body: JSON.stringify(testData)
        });

        const result = await response.json();
        
        if (response.ok) {
            console.log('âœ… Batch processing completed successfully!');
            console.log('ğŸ“Š Results:', {
                message: result.message,
                ocrMethod: result.ocrMethod,
                chunksProcessed: result.chunksProcessed,
                totalChunks: result.totalChunks,
                gcsUrisUploaded: result.gcsUrisUploaded,
                indexingCompleted: result.indexingCompleted
            });
        } else {
            console.error('âŒ Batch processing failed:', result.error);
        }

    } catch (error) {
        console.error('âŒ Test failed:', error);
    }
}

// Instructions for running the test
console.log(`
ğŸ“‹ Instructions for testing Document AI Batch Processing:

1. Make sure your Next.js development server is running:
   npm run dev

2. Update the testData object above with:
   - fileName: Name of a PDF file in your Supabase storage
   - bucketName: Your Supabase storage bucket name
   - uploadPath: Full path to the PDF file in storage
   - userId: A valid user ID

3. Ensure your Google Cloud Storage bucket 'paradigms-documents' exists and is accessible

4. Run this test:
   node test-batch-processing.js

ğŸ”§ Key changes in the batch processing implementation:

âœ… PDF chunks are uploaded to Google Cloud Storage first
âœ… Document AI batch processing processes all chunks at once
âœ… Results are downloaded and combined automatically
âœ… More efficient than processing chunks individually
âœ… Better error handling and logging
âœ… Supports larger documents through proper batching

ğŸ“ Google Cloud Storage structure:
   paradigms-documents/
   â”œâ”€â”€ chunks/
   â”‚   â””â”€â”€ {userId}/
   â”‚       â”œâ”€â”€ document_chunk_1.pdf
   â”‚       â”œâ”€â”€ document_chunk_2.pdf
   â”‚       â””â”€â”€ ...
   â””â”€â”€ batch_output/
       â””â”€â”€ {userId}/
           â””â”€â”€ {timestamp}/
               â”œâ”€â”€ result_1.json
               â”œâ”€â”€ result_2.json
               â””â”€â”€ ...
`);

// Uncomment the line below to run the test
// testBatchProcessing();
